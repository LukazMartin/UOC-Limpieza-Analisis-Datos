---
title: 'PRÁCTICA 2: ¿Cómo realizar la limpieza y análisis de datos?'
author: "Lukaz Martin Doehne y Pablo Vadillo Berganza"
date: "07/01/2023"
output:
  pdf_document:
    toc: true
    toc_depth: 3
lang: es
tuthor: Laia Subirats Maté
---

\pagebreak

# 1.Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?

En primer lugar, cargaremos los datos con los que trabajar.

```{r echo=FALSE}
df <- read.csv('../data/heart.csv')
```

Nuestro dataset está compuesto por 303 registros y 14 variables. Contiene información clínica de pacientes con el fin de poder predecir la probabilidad de fallo cardíaco. 


Para ello, la variable a predecir, "output", contiene información acerca del estrechamiento de los vasos sanguíneos obtenida a través de una angiografía. Si el estrechamiento es inferior al 50% toma el valor 0 y por tanto el paciente no se considera en riesgo de padecer una enfermedad del corazón. Por el contrario, si el estrechamiento es superior al 50%, la variable tomará el valor 1 y el paciente tendrá una mayor probabilidad de ataque al corazón.


A partir de aquí, será interesante realizar un análisis del resto de campos para ver cuáles son determinantes en la predicción del fallo cardíaco.


Pero antes, vamos a realizar una breve exploración del dataset.

```{r}
str(df)
```

Todas las variables son numéricas.

```{r}
summary(df)
```

Para terminar con este apartado, vamos a explicar brevemente todos y cada uno de los campos del dataset:

  · Age: Edad de los pacientes en años. Toma valores entre 29 y 77. La media es 54,37.
  
  · Sex: Sexo de los pacientes. (1 = hombre; 0 = mujer).
  
  · Cp: Dolor de pecho. (1 = angina típica; 2 = angina atípica; 3 = dolor no anginal; 4 = asintomático).
  
  · Trtbps: Presión arterial en reposo. Se trata del valor tomado en el ingreso al hospital, en mm Hg. Toma valores entre 94 y 200, siendo la media de 131,6.
  
  · Chol: Colesterol sérico. Medido en mg/dl. Toma valores entre 126 y 564, con una media de 246,3.
  
  · Fbs: Nivel de azúcar en sangre en ayunas. (1 = > 120 mg/dl; 0 = <= 120 mg/dl).
  
  · Restecg: Resultados del electrocardiograma en reposo. (0 = normal; 1 = onda ST-T anómala; 2 = hipertrofia ventricular izquierda).
  
  · Thalachh: Máximo pulso cardíaco obtenido. Toma valores entre 71 y 202. La media es 149,6.
  
  · Exng: Angina inducida del ejercicio. (1 = sí; 0 = no).
  
  · Oldpeak: Depresión del segmento ST inducida por ejercicio relativo al descanso. Toma valores entre 0 y 6,2, con una media de 1,04.
  
  · Slp: Pendiente del segmento ST del pico del ejercicio. (1 = ascendente; 2 = plano; 3 = descendente).
  
  · Caa: Número de los principales vasos sanguíneos coloreados por la fluoroscopia. Toma valores entre 0 y 4, con una media de 0,73.
  
  · Thall: Talasemia. Menor nivel de hemoglobina. (1 = defecto fijo; 2 = normal; 3 = defecto reversible). El defecto fijo hace referencia a un defecto que ocurre tanto en reposo como durante el esfuerzo. El defecto reversible, por el contrario, hace referencia a un defecto que ocurre durante el esfuerzo que no existía durante el reposo.

  · Output: La variable a predecir. Diagnóstico de fallo cardíaco (0 = Estrechamiento de vasos sanguíneos < 50%; 1 = Estrechamiento de vasos sanguíneos > 50%).

# 2.Integración y selección 

En este apartado analizaremos qué campos son realmente significativos a la hora de predecir la variable output.

En primer lugar, calcularemos los coeficientes de correlación de Pearson, y nos fijaremos expresamente en la variable a predecir, output, con el resto de variables independientes.

```{r}
cor_pearson <- cor(df)

# Mostramos los valores de output con el resto,

cor_pearson[14,]
```

Las variables chol (colesterol sérico) y fbs (nivel de azúcar en sangre en ayunas) presentan un coeficiente por debajo del 0.1 (en valor absoluto), por lo que podemos concluir que la correlación con la variable a predecir es prácticamente inexistente, y podemos prescindir de estas variables de cara al análisis.


Para acabar con este apartado, haremos un análisis de componentes principales, para detectar cuáles son las variables que realmente describen el conjunto de datos.

Pero antes, descargaremos las librerías necesarias,

```{r echo=TRUE, message=FALSE, warning=FALSE}
#if (!require('FactoMineR')) install.packages('FactoMineR'); library('FactoMineR')
#if (!require('factoextra')) install.packages("factoextra"); library('factoextra')
```

A continuación, realizamos el test de PCA,

```{r}
#PCA <- PCA(df, graph=FALSE)

# Contribuciones de las variables a PC1,
#fviz_contrib(PCA, choice = "var", axes = 1, top = 14)
# Contribuciones de las variables a PC2,
#fviz_contrib(PCA, choice = "var", axes = 2, top = 14)
```

En estos gráficos podemos ver el nivel de contribución de cada una de las variables a las 2 primeras dimensiones de las componentes principales. Seleccionamos las dos primeras dimensiones ya que las variables que estén correlacionadas con éstas serán las que sean capaces de explicar todo el dataset.


Las variables output, oldpeak, thalachh, exng, slp y cp contribuyen más que la media de contribución esperada (línea roja) para la dimensión 1. Sin embargo, caa, age y thall también contribuyen de forma no despreciable. Las variables trtbps, sex, restecg, chol y fbs son las que menos contribuyen a la dimensión 1.


Si vamos a la dimensión 2, tenemos que age, trtbps, sex, chol, fbs y cp contribuyen más que la media de contribución esperada.

Viendo la perspectiva para las dos primeras dimensiones, vemos que la variable restecg (Resultado de electrocardiograma en reposo) es la que menos contribuye. Además, volviendo al coeficiente de correlación de Pearson, después de chol y fbs, también es la que menor coeficiente, en valor absoluto, presenta.


En conclusión, prescindiremos de las variables chol y fbs por presentar una correlación prácticamente inexistente con la variable a predecir, output. Por otro lado, también prescindimos del campo restecg ya que es el que menos contribuye a la variabilidad del dataset, además de presentar una correlación débil con la variable a predecir.


```{r}
col <- c(1,2,3,4,8,9,10,11,12,13,14)

df_def <- df[,col]
```



\pagebreak
# 3. Limpieza de los datos

## 3.1. ¿Los datos contienen ceros o elementos vacíos? 

Ante elementos vacíos se pueden seguir principalmente dos estrategias: la **imputación** y la **eliminación**.

Para la primera, se trata de rellenar los valores nulos basandose en los valores no vacíos. Esto se realizar típicamente mediante *knn*, interpolación o cogiendo la media de una columna.

La segunda estrategía consiste en eliminar las filas con valores nulos. Esta se suele usar cuando se tiene un dataset grande y los valores nulos no suponen un gran porcentaje.

Podemos comprobar que no se encuentran elementos nulos en el csv con el siguiente código:

```{r}
sapply(df, function(x) sum(is.na(x)))
```

Dado que no encontramos valores nulos no aplicamos ninguna de las estrategias. Pero vamos a discretizar la columna *age* para mejor procesamiento.

**Discretización**

Para la variable edad, en lugar de tener el valor exacto crearemos grupos por decenas de edad.

E.g:

·30-39 -> 3

·40-49 -> 4

...

```{r echo=FALSE}
df$age[df$age >= 20 & df$age<30] = 2
df$age[df$age >= 30 & df$age<40] = 3
df$age[df$age >= 40 & df$age<50] = 4
df$age[df$age >= 50 & df$age<60] = 5
df$age[df$age >= 60 & df$age<70] = 6
df$age[df$age >= 70 & df$age<80] = 7
```

Vemos el resultado


```{r echo=FALSE}
table(df$age)
```



## 3.2. Identifica y gestiona los valores extremos

Los valores extremos (*outliers*) los identificaremos como aquellos que se encuentran fuera del rango [Lo,Ho] donde:

  · Lo = Q1 – (1.5 * IQR) 
  
  · Ho = Q3 + (1.5 * IQR)
  
y IQR = Q3 - Q1. También hay una fórmula que trata outliers los que sobrepasan la media +/- la desviación estándar, pero optaremos por la fórmula descrita, ya que también es la que usa *R* por defecto en la función boxplot.

A continuación, mostramos la cantidad de outliers por columna con la siguiente función:

```{r}
outliers<-function(x){
  outliers<-boxplot.stats(x)$out
  return (length(outliers))
}
```

```{r echo=FALSE}
sapply(df, function(x) outliers(x))
```

Encontramos varios outliers en los datos. Las columnas *fbs*, *caa* y *thall* son columnas **númericas discretas** por lo que no los consideramos outliers, sino solo un dataset desequilibrado.

Visualizamos para entender mejor las demás variables.

```{r echo=FALSE}
boxplot(df$trtbps,df$chol,df$thalachh, names =c("trtbps","chol","thalachh"), col=c("blue","red","green"))
```

La variable *oldpeak* la visualizamos en un histograma al estar en una escala diferente a las demás.

```{r echo=FALSE}
h<-hist(df$oldpeak,plot=FALSE)
plot(h, xaxt = "n", xlab = "Oldpeak Histogram", ylab = "Counts", main = "", col = "pink")
```


Viendo las visualizaciones suponemos que no ha habido errores a la hora de capturar los datos. Si que es cierto que en la columna *chol* encontramos un valor muy alejado del valor medio, pero lo mantendremos dentro del analisis al igual que el resto de los outliers. Si creamos un modelo de predicción podemos asegurar de incluir los outliers tanto en el train como en el test set.
\pagebreak

# 4. Análisis de los datos

## 4.1. Selección de los grupos de datos que se quieren analizar/compara

Hay 3 grupos en especial que nos interesa estudiar en función del fallo cardíaco:

· La variable **sex**. En función de hombres y mujeres.

· La variable **age**. En función de la edad de los pacientes.

· La variable **trtbps**. En función de la presión arterial de los pacientes. Información extraída de: [mayoclinic.org](https://www.mayoclinic.org/es-es/diseases-conditions/high-blood-pressure/diagnosis-treatment/drc-20373417#:~:text=Presi%C3%B3n%20arterial%20normal,de%20emergencia%20local.)


```{r}
# Agrupación por género
df.male <- df[df$sex == 1,]
df.female <- df[df$sex == 0,]
# Agrupación por edad
df.young_adult <- df[df$age<4,]
df.adult <- df[df$age>=4&df$age<6,]
df.old_adult <- df[df$age>=6,]
# Agrupación por presión arterial
df.normal_pressure <- df[df$trtbps<120,]
df.high_pressure <- df[df$trtbps>=120&df$trtbps<130,]
df.hipertension1 <- df[df$trtbps>=130&df$trtbps<140,]
df.hipertension2 <- df[df$trtbps>=140&df$trtbps<180,]
df.crisis_hipertension <- df[df$trtbps>=180,]
```




## 4.2. Comprobación de la normalidad y homogeneidad de la varianza

## 4.3. Aplicación de pruebas estadísticas para comparar los grupos de datos

# 5. Representación de los resultados a partir de tablas y gráficas

# 6. Resolución del problema

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars, echo=FALSE}
summary(cars)
```

